---  
share: ture  
tags:  
  - notes  
  - ä¼˜åŒ–  
  - åˆ†æç­–ç•¥  
  - PPT  
links:  
  - - ""  
date: 2023-03-12 13:58  
status: â­•  
aliases: []  
---  
  
![|banner](../assets/img/23.jpg)  
  
# è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜å ç”¨åˆ†æåŠæ€§èƒ½ä¼˜åŒ–  
---  
ğŸ“Œ #ä¼˜åŒ– #åˆ†æç­–ç•¥   
 > [!abstract]+   
 > æ¢³ç†é¢ç›¸é«˜æ•ˆè®­ç»ƒçš„æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ã€‚  
  
## è®¡ç®—æ€§èƒ½åˆ†æ  
![[../assets/img/Pasted image 20230311174306.png|500]]  
- [i] ä»æ•´ä¸ªæ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹æ¥çœ‹ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚å¤šæ•°ä¼šå¯¼è‡´æ˜¾å­˜æˆä¸ºæœ€ç»ˆæ€§èƒ½çš„ç“¶é¢ˆã€‚  
![[../assets/img/Pasted image 20230312140259.png|Pasted image 20230312140259.png]]  
  
 > [!TIP]- ğŸŒ° ä¸¾ä¸ªä¾‹å­  
 > ä»¥VGG16ä¸ºä¾‹ï¼š  
 > - æ¨¡å‹çš„å‚æ•°é‡ä¸º138Mï¼ŒåŠ è½½å ç”¨æ˜¾å­˜ï¼š138 * 4 / 1024 / 1024 = 528MB   
 > - å¦‚æœä½¿ç”¨SGDä½œä¸ºä¼˜åŒ–å™¨ï¼Œéœ€è¦é¢å¤–çš„å‚æ•°æ¢¯åº¦ä¹Ÿä¸º528MB   
 >  - å¦‚æœä½¿ç”¨Adamï¼Œé¢å¤–çš„æ¢¯åº¦ç›¸å…³æ•°æ®ä¸º 528* 3 MB   
 > - æ¯å±‚è¾“å‡ºæ‰€å ç”¨çš„å†…å­˜å’Œbatch sizeæˆæ­£æ¯”ï¼š  
 >  - bs=1,éœ€è¦58.12MBï¼Œè€Œè®­ç»ƒè¿‡ç¨‹æœ‰forwardçš„passwordï¼Œæ‰€ä»¥ä¸º58.12MB x 2   
 >  - ![](https://upload-images.jianshu.io/upload_images/10780978-c0e762f4c98bcb71.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720)  
  
### é—®é¢˜åˆ†æ  
  
![[../assets/img/Diagram 3.svg|500]]  
  
![[../assets/img/gpu_memory.svg|500]]  
  
 > [!FAQ]- æ€è€ƒ  
 > 1ï¼‰å“ªä¸ªç¯èŠ‚æ˜¾å­˜å ç”¨æœ€å¤šï¼Ÿ  
 > 2ï¼‰å“ªä¸ªç¯èŠ‚æ˜¯è®¡ç®—æ¡†æ¶/å¹³å°å¯ä»¥ä¼˜åŒ–çš„æ˜¾å­˜å ç”¨ï¼Ÿ å“ªä¸ªç¯èŠ‚æ˜¯ç”¨æˆ·ä¾§éœ€è¦ä¸»å¯¼çš„ï¼Ÿ  
 > 3ï¼‰å“ªäº›æ˜¯è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå­˜åœ¨å·®å¼‚çš„ï¼Ÿ	  
  
  
### æ¨¡å‹æƒé‡  
-   4 bytes * number of parameters for fp32 training  
-   6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)  
  
### ä¼˜åŒ–å™¨çŠ¶æ€   
-   8 bytes * number of parameters for normal AdamW (maintains 2 states)  
-   2 bytes * number of parameters for 8-bit AdamW optimizers likeÂ [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)  
-   4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)  
  
### æ¢¯åº¦   
-   4 bytes * number of parameters for either fp32 mixed precision training (gradients are always kept in fp32)  
  
### å‰å‘æ¿€æ´»ä¿¡æ¯(ä¸­é—´ç»“æœ)  
-   size depends on many factors, the key ones being sequence length, hidden size and batch size.  
- There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation.  
  
### ä¸´æ—¶å­˜å‚¨   
- Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding itâ€™s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.  
  
### å‡½æ•°æœ‰å…³çš„å†…å­˜å ç”¨  
Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs.  
  
  
  
 > [!QUOTE]+ æ˜¾å­˜æ¶ˆè€—æ•°æ®   
 > æ¨¡å‹è®­ç»ƒç›¸å…³ï¼š **å¯¹äºGPT-2è¿™æ ·1.5Bå‚æ•°çš„æ¨¡å‹ï¼Œæ˜¾å­˜æ¶ˆè€—è‡³å°‘24GBã€‚**  
 >  ä¸­é—´å˜é‡çš„æ¶ˆè€—ï¼ˆæ¿€æ´»ï¼‰ï¼šä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œæ¨¡å‹ä¸º1.5Bçš„GPT-2ï¼Œåºåˆ—é•¿åº¦ä¸º1Kï¼Œbatch sizeä¸º32ï¼Œåˆ™æ¶ˆè€—æ˜¾å­˜ä¸º60GBã€‚  
 >  **ä¸´æ—¶ç¼“å­˜åŒº(Temporary buffers)**ã€‚å¯¹äºå¤§æ¨¡å‹ï¼Œç”¨äºå­˜å‚¨ä¸­é—´ç»“æœçš„ä¸´æ—¶bufferä¹Ÿä¼šæ¶ˆè€—å¤§é‡æ˜¾å­˜ã€‚ä¾‹å¦‚åœ¨all-reduceæ—¶ï¼Œéœ€è¦ä¸€ä¸ªå¹³å¦çš„bufferæ¥èåˆæ‰€æœ‰çš„æ¢¯åº¦ï¼Œä»è€Œæ”¹å–„ååé‡ã€‚ä¾‹å¦‚ï¼Œè·¨è®¾å¤‡çš„all-reduceæ“ä½œä¼šéšç€æ¶ˆæ¯çš„å¢å¤§è€Œå¢åŠ ã€‚è™½ç„¶ï¼Œæ¢¯åº¦æœ¬æ–‡æ˜¯fp16çš„å¼ é‡ï¼Œä½†æ˜¯æœ‰äº›æ“ä½œä¸­å¯èƒ½éœ€è¦èåˆçš„bufferä¸ºfp32ã€‚å½“æ¨¡å‹å°ºå¯¸å¾ˆå¤§æ—¶ï¼Œä¸´æ—¶çš„bufferä¹Ÿä¸å°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº1.5Bå‚æ•°çš„æ¨¡å‹ï¼Œä¸€ä¸ªfp32çš„bufferéœ€è¦6GBçš„æ˜¾å­˜ã€‚  
 >  **æ˜¾å­˜ç¢ç‰‡**ã€‚å³ä½¿åœ¨æœ‰è¶³å¤Ÿæ˜¾å­˜çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯èƒ½ä¼šå¯¼è‡´Out of Memoryï¼Œè¿™æ˜¯ç”±äºæ˜¾å­˜ç¢ç‰‡å¯¼è‡´çš„ã€‚åœ¨è¿›ç¨‹å‘å‡ºæ˜¾å­˜è¯·æ±‚æ—¶ï¼Œå¦‚æœæ²¡æœ‰è¿ç»­çš„æ˜¾å­˜æ¥æ»¡è¶³è¯·æ±‚ï¼Œå³ä½¿æ€»çš„æ˜¾å­˜ä»ç„¶è¶³å¤Ÿï¼Œè¯¥è¯·æ±‚ä¹Ÿä¼šå¤±è´¥ã€‚å½“è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹æ—¶ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æ˜æ˜¾çš„æ˜¾å­˜ç¢ç‰‡ã€‚æç«¯æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå¯¼è‡´30%çš„æ˜¾å­˜ç¢ç‰‡ã€‚  
  
 #PPT   
 http://home.ustc.edu.cn/~fzr/pdf/GPU_memory_analysis.pdf  
 https://www.bilibili.com/read/cv19522369?from=articleDetail  
```cardlink  
url: https://www.bilibili.com/read/cv19522369?from=articleDetail  
title: "ä¸æ­¢äºZeROï¼šBMTrainæŠ€æœ¯åŸç†æµ…æ"  
description: "ä¸ç°æœ‰çš„å¤§æ¨¡å‹è®­ç»ƒä½¿ç”¨ç™¾ä½™å¼ æ˜¾å¡ç›¸æ¯”ï¼Œæˆ‘ä»¬å‘èµ·çš„CPM-Live å¼€æºå¤§æ¨¡å‹ç›´æ’­è®­ç»ƒå®ç°äº† 8 å¼  A100 æ˜¾å¡ è®­ç»ƒç™¾äº¿å¤§æ¨¡å‹ã€‚è¿™ä¼˜å¼‚æ•ˆæœçš„èƒŒååŸºäºçš„æ˜¯ å¤§æ¨¡å‹é«˜æ•ˆè®­ç»ƒå·¥å…· BMTrain å’Œ æ¨¡å‹ä»“åº“ ModelCenterã€‚ä¸ç°æœ‰æ¡†æ¶ç›¸æ¯”ï¼ŒBMTrain èƒ½å¤Ÿå®ç°å¤§æ¨¡å‹çš„ä½èµ„æºã€é«˜æ•ˆè®­ç»ƒï¼Œå¹¶ä¸”ç®€å•æ˜“ç”¨ï¼Œä¾¿äºå¼€å‘è€…ä¸Šæ‰‹ã€‚æ”¯æ’‘èµ· BMTrain ä¼˜å¼‚æ€§èƒ½è¡¨ç°çš„æ˜¯å…¶é‡‡ç”¨çš„å¤šé¡¹åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–æŠ€æœ¯ï¼Œå®ƒä»¬å…±åŒè§£å†³äº†å¤§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ æ˜¾å­˜å ç”¨ é—®é¢˜ã€‚ä¸ºäº†æ·±åˆ»ç†è§£è¿™ä¸€å…³é”®é—®é¢˜ï¼Œæˆ‘ä»¬ä¸å¦¨åˆ†æä¸€ä¸‹æ¨¡å‹è®­ç»ƒ"  
host: www.bilibili.com  
image: https://i0.hdslb.com/bfs/article/6f3db3c5ccab1ce35f19d8601c6b718ebe37123f.png  
```  
  
  
  
## åŸºäºPytorchçš„å®éªŒåˆ†æ  
### è®¡ç®—å·¥å…·  
```bash  
pip install torchstat thop  
  
from torchstat import stat   
stat(model, (3, 224, 224))  
```  
  
```python  
from pynvml import *  
  
def print_gpu_utilization():  
    nvmlInit()  
    handle = nvmlDeviceGetHandleByIndex(0)  
    info = nvmlDeviceGetMemoryInfo(handle)  
    print(f"GPU memory occupied: {info.used//1024**2} MB.")  
```  
  
- `torch.cuda.memory_allocated()` å’Œ `torch.cuda.max_memory_allocated()`  
- å‰è€…å¯ä»¥ç²¾å‡†åœ°åé¦ˆå½“å‰è¿›ç¨‹ä¸­Torch.Tensoræ‰€å ç”¨çš„GPUæ˜¾å­˜ï¼Œåè€…åˆ™å¯ä»¥å‘Šè¯‰æˆ‘ä»¬åˆ°è°ƒç”¨å‡½æ•°ä¸ºæ­¢æ‰€è¾¾åˆ°çš„æœ€å¤§çš„æ˜¾å­˜å ç”¨å­—èŠ‚æ•°ã€‚  
 -   ä½¿ç”¨torch.cuda.memory_allocated() å¯ä»¥çœ‹åˆ°å½“å‰Tensorå ç”¨çš„æ˜¾å­˜  
 -   ä½¿ç”¨ torch.cuda.memory_reserved() å¯ä»¥çœ‹åˆ°æ€»å…±å ç”¨çš„æ˜¾å­˜  
 -   ä½¿ç”¨ torch.cuda.empty_cache() æ¸…ç©ºæœªä½¿ç”¨çš„ç¼“å­˜ï¼Œä½†æ˜¯å·²ç»ä½¿ç”¨çš„æ˜¯ä¸èƒ½é‡Šæ”¾  
- torchsummary å’Œ torchinfoéƒ½å¯ä»¥å±•ç°æ¨¡å‹çš„å¤§å°ä¿¡æ¯  
### Pytorchä¸­æ˜¾å­˜å ç”¨åˆ†æ  
  
- [?] nvidia-smiä¸­çœ‹åˆ°çš„å ç”¨ = `CUDA Context + pytorchçš„ç¼“å­˜åŒº` = `CUDA Context + æœªä½¿ç”¨çš„ç¼“å­˜ + å·²ä½¿ç”¨çš„ç¼“å­˜`   
  
åœ¨PyTorchä¸­ï¼Œæ˜¾å­˜æ˜¯æŒ‰é¡µä¸ºå•ä½è¿›è¡Œåˆ†é…çš„ï¼Œè¿™å¯èƒ½æ˜¯CUDAè®¾å¤‡çš„é™åˆ¶ã€‚å°±ç®—æˆ‘ä»¬åªæƒ³ç”³è¯·4å­—èŠ‚çš„æ˜¾å­˜ï¼Œpytorchä¹Ÿä¼šå…ˆå‘CUDAè®¾å¤‡ç”³è¯·2MBçš„æ˜¾å­˜åˆ°è‡ªå·±çš„cacheåŒºä¸­ï¼Œç„¶åpytorchå†ä¸ºæˆ‘ä»¬åˆ†é…512å­—èŠ‚æˆ–è€…1024å­—èŠ‚çš„ç©ºé—´ã€‚è¿™ä¸ªåœ¨ä½¿ç”¨`torch.cuda.memory_allocated()`çš„æ—¶å€™å¯ä»¥çœ‹å‡ºæ¥512å­—èŠ‚ï¼›ç”¨`torch.cuda.memory_cached()`å¯ä»¥çœ‹å‡ºå‘CUDAç”³è¯·çš„2MBã€‚ç›´è§‚ç‚¹æ¥è¯´ï¼Œçœ‹å›¾å§ï¼ŒPyTorchçš„æ˜¾å­˜ç®¡ç†æ˜¯ä¸€ä¸ªå±‚çº§ç»“æ„ã€‚  
åœ¨PyTorchä¸­ï¼Œåªè¦ä¸€ä¸ªTensorå¯¹è±¡åœ¨åç»­ä¸ä¼šå†è¢«ä½¿ç”¨ï¼Œé‚£ä¹ˆPyTorchå°±ä¼šè‡ªåŠ¨å›æ”¶è¯¥Tensoræ‰€å ç”¨çš„æ˜¾å­˜ï¼Œå¹¶ä»¥ç¼“å†²åŒºçš„å½¢å¼ç»§ç»­å ç”¨æ˜¾å­˜ã€‚  
![[../assets/img/Pasted image 20230108101533.png|450]]  
  
> Pytorchçš„æœºåˆ¶æ˜¯ä½¿ç”¨ç¼“å­˜åˆ†é…å™¨æ¥ç®¡ç†ç¼“å­˜åˆ†é…çš„(å› ä¸ºè¿™æ ·é€Ÿåº¦å¿«), ä½†æ˜¯åœ¨ç¼“å­˜åˆ†é…å™¨çš„æœºåˆ¶ä¸‹,Â **ä¸€ä¸ªTensorå°±ç®—è¢«é‡Šæ”¾äº†ï¼Œè¿›ç¨‹ä¹Ÿä¸ä¼šæŠŠç©ºé—²å‡ºæ¥çš„æ˜¾å­˜è¿˜ç»™GPUï¼Œè€Œæ˜¯ç­‰å¾…ä¸‹ä¸€ä¸ªTensoræ¥å¡«å…¥è¿™ä¸€ç‰‡è¢«é‡Šæ”¾çš„ç©ºé—´(å³åªè¦ä¸€ä¸ªTensorå¯¹è±¡åœ¨åç»­ä¸ä¼šå†è¢«ä½¿ç”¨ï¼Œé‚£ä¹ˆPyTorchå°±ä¼šè‡ªåŠ¨å›æ”¶è¯¥Tensoræ‰€å ç”¨çš„æ˜¾å­˜ï¼Œå¹¶ä»¥ç¼“å†²åŒºçš„å½¢å¼ç»§ç»­å ç”¨æ˜¾å­˜ï¼Œæ‰€ä»¥åœ¨nvidia-smi/gpustatä¸­çœ‹åˆ°çš„æ˜¾å­˜å¹¶æ²¡æœ‰å‡å°‘**  
  
  
 > [!FAQ]+ æ··åˆç²¾åº¦è®¡ç®—ä¸­Zeroè®ºæ–‡ä¸­çš„è®¡ç®—å…¬å¼Adamçš„ä¼˜åŒ–å™¨çŠ¶æ€ä¸º12B  
>  å› ä¸ºä½¿ç”¨äº†æ··åˆç²¾åº¦ï¼ŒåŒ…æ‹¬æ¨¡å‹å‚æ•°ï¼ˆfp16ï¼‰ã€æ¨¡å‹æ¢¯åº¦ï¼ˆfp16ï¼‰å’ŒAdamçŠ¶æ€ï¼ˆfp32çš„æ¨¡å‹å‚æ•°å¤‡ä»½ï¼Œfp32çš„momentumå’Œfp32çš„varianceï¼‰ï¼›  
  >  ![[../assets/img/Pasted image 20230108115706.png|Pasted image 20230108115706.png]]  
  
 > [!TIP]- CUDA Context  
 > å°±æ˜¯åœ¨ç¬¬ä¸€æ¬¡æ‰§è¡ŒCUDAæ“ä½œï¼Œä¹Ÿå°±æ˜¯ä½¿ç”¨GPUçš„æ—¶å€™æ‰€éœ€è¦åˆ›å»ºçš„ç»´æŠ¤è®¾å¤‡é—´å·¥ä½œçš„ä¸€äº›ç›¸å…³ä¿¡æ¯ã€‚  
 > è¿™ä¸ªå€¼è·ŸCUDAçš„ç‰ˆæœ¬ï¼Œpytorchçš„ç‰ˆæœ¬ä»¥åŠæ‰€ä½¿ç”¨çš„è®¾å¤‡éƒ½æ˜¯æœ‰å…³ç³»çš„ã€‚æ¯”å¦‚3090ç”¨çš„CUDA 11.4ï¼Œå¼€é”€ä¸º1639MBï¼›V100ç”¨çš„CUDA 10.2ï¼Œå¼€é”€ä¸º1351MBã€‚	  
  
### å®éªŒè„šæœ¬  
  
1. æ¨¡å‹å®šä¹‰ --> 2. å‰å‘ä¼ æ’­ --> 3.åå‘ä¼ æ’­ -->  4.å‚æ•°æ›´æ–°  
  
  
```python  
import torch  
```  
  
#### åˆ†é…ä¸€ä¸ªç®€å•çš„Tensorçš„æƒ…å†µ  
å°†æ˜¾å­˜ä»nvidia-smiä¸­é‡Šæ”¾ï¼š  
```python  
torch.cuda.empty_cache()  
```  
  
```python  
print(torch.cuda.memory_allocated())  
```  
    0  
  
```python  
temp = torch.tensor([1.0]).cuda()  
```  
é¡µåˆ†é…: Pytorchæœ‰è‡ªå·±çš„æ˜¾å­˜ç®¡ç†ç³»ç»Ÿï¼Œé€šè¿‡é¢„ç•™å’Œé¡µåˆ†é…æ¥ç®¡ç†å†…å­˜  
  
```python  
print(torch.cuda.memory_allocated())  
```  
  
    512  
  
CUDA Contextï¼š å’Œä¸åŒçš„æ˜¾å¡ã€è®¡ç®—æ¡†æ¶çš„ç‰ˆæœ¬æœ‰å…³(æ‰§è¡Œä¸€ä¸ªç®€å•æŒ‡ä»¤ä¹‹åï¼ŒæŸ¥çœ‹nvidia-smiçœ‹å®é™…åˆ†é…å€¼ï¼Œå‡å»`torch.cuda.memory_reserved() `çš„å€¼)ï¼Œ å¯èƒ½æ˜¯ä¸ªåŠ¨æ€çš„å€¼å¤§æ¦‚600å¤šMB-1GB  
  
ä¸åŒçš„pytorchå‡½æ•°ï¼Œæ˜¾å¡å‹å·ï¼Œé©±åŠ¨ï¼Œæ“ä½œç³»ç»Ÿï¼Œcudaç‰ˆæœ¬éƒ½æ˜¯ä¼šå½±å“context_memoryå¤§å°çš„ã€‚  
  
```python  
torch.cuda.memory_reserved()  
```  
    2097152  
```python  
torch.cuda.memory_reserved() / 1024 / 1024  
```  
    2.0  
  
```python  
del temp  
```  
  
```python  
torch.cuda.empty_cache()  
```  
```python  
print(torch.cuda.memory_allocated())  
```  
  
    512  
  
  
#### ä¸€ä¸ªæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜æ•°æ®  
  
  
```python  
import torch  
```  
  
```python  
current_mem = 0  
```  
  
```python  
def get_mem_total():  
    global current_mem  
    current_mem = torch.cuda.memory_allocated() / 1024 / 1024  
    return current_mem  
```  
  
  
```python  
get_mem_total()  
```  
  
  
    0.0  
  
  
```python  
def get_mem_diff():  
    old = current_mem  
    total = get_mem_total()  
    return (total - old)  
```  
  
  
```python  
class Model(torch.nn.Module):  
    def __init__(self):  
        super().__init__()  
        self.model = torch.nn.Linear(1024, 1024, bias=False, dtype=torch.float16)  
      
    def forward(self, x):  
        for i in range(10):  
            x = self.model(x)  
        return x  
```  
  
  
```python  
model = Model().cuda()  
```  
  
  
```python  
print(f"Model parameters: {get_mem_diff()} MB" )  
```  
  
    Model parameters: 2.0 MB  
  
  
  
```python  
inputs = torch.zeros((10240, 1024),  dtype=torch.float16).cuda()  
```  
  
  
```python  
print(f"Inputs: {get_mem_diff()} MB" )  
```  
  
    Inputs: 20.0 MB  
  
  
  
```python  
outputs = model(inputs)   
```  
  
  
```python  
print(f"Outpus and buffers: {get_mem_diff()} MB" )  
```  
  
    Outpus and buffers: 40.0 MB  
  
  
  
```python  
loss = torch.sum(outputs) # + 512(æœ€å°åˆ†é…å•ä½)  
```  
  
  
```python  
get_mem_total()  
```  
  
  
  
  
    62.00048828125  
  
  
  
  
```python  
loss.backward()  
```  
  
  
```python  
print(f"After backward: {get_mem_total()} MB")  
```  
  
    After backward: 44.00048828125 MB  
  
  
åŒ…æ‹¬ï¼š20MBçš„è¾“å…¥ä¿¡æ¯ï¼Œ20MBçš„è¾“å‡ºï¼Œä»¥åŠ2MBçš„æ¨¡å‹å‚æ•°ï¼Œ2MBçš„æ¨¡å‹å‚æ•°æƒé‡ï¼Œè¿˜æœ‰ä¸€äº›å…¶å®ƒä¸´æ—¶å˜é‡  
  
  
```python  
with torch.no_grad():  
    outputs = model(inputs)   
```  
  
  
```python  
print(f"Use a eval mode for predict: {get_mem_diff()} MB")  
```  
  
    Use a eval mode for predict: 0.0 MB  
  
  
  
```python  
!pip install torchinfo  
```  
  
  
```python  
from torchinfo import summary  
```  
  
```python  
summary(model)  
```  
  
    =================================================================  
    Layer (type:depth-idx)                   Param #  
    =================================================================  
    Model                                    --  
    â”œâ”€Linear: 1-1                            1,048,576  
    =================================================================  
    Total params: 1,048,576  
    Trainable params: 1,048,576  
    Non-trainable params: 0  
    =================================================================  
  
  
```python  
summary(model, (10240, 1024), dtypes=[torch.float16])  
```  
  
    /home/admin/.local/lib/python3.8/site-packages/torchinfo/torchinfo.py:395: UserWarning: Half precision is not supported with input_size parameter, and may output incorrect results. Try passing input_data directly.  
      warnings.warn(  
      
    ==========================================================================================  
    Layer (type:depth-idx)                   Output Shape              Param #  
    ==========================================================================================  
    Model                                    [10240, 1024]             --  
    â”œâ”€Linear: 1-1                            [10240, 1024]             1,048,576  
    â”œâ”€Linear: 1-2                            [10240, 1024]             (recursive)  
    â”œâ”€Linear: 1-3                            [10240, 1024]             (recursive)  
    ==========================================================================================  
    Total params: 1,048,576  
    Trainable params: 1,048,576  
    Non-trainable params: 0  
    Total mult-adds (T): 1.07  
    ==========================================================================================  
    Input size (MB): 20.97  
    Forward/backward pass size (MB): 41.94  
    Params size (MB): 2.10  
    Estimated Total Size (MB): 65.01  
    ==========================================================================================  
  
#### å¼•å…¥ä¼˜åŒ–å™¨  
  
å¦‚ä¸‹Adamå¼•å…¥äº†ä¸¤ä¸ªæ–°çš„çŠ¶æ€å‚æ•°ï¼šåŠ¨é‡å’Œå‡å€¼  
  
SGDåŸºç¡€ä¸Šï¼Œä¸ºæ¯ä¸ªå‚æ•°æ¢¯åº¦å¢åŠ äº†ä¸€é˜¶åŠ¨é‡ï¼ˆmomentumï¼‰å’ŒäºŒé˜¶åŠ¨é‡ï¼ˆvarianceï¼‰  
  
  
```python  
optimizer = torch.optim.Adam(model.parameters())  
```  
  
  
```python  
optimizer.step()  
```  
  
  
```python  
print(f"Optimizer: {get_mem_diff()} MB" )  
```  
  
    Optimizer: 4.0 MB  
  
```python  
torch.cuda.max_memory_allocated()  
```  
  
ç”±äºè®¡ç®—æœºè®¡ç®—çš„ç‰¹æ€§ï¼Œæœ‰ä¸€äº›è®¡ç®—æ“ä½œåœ¨è®¡ç®—è¿‡ç¨‹ä¸­æ˜¯ä¼šå¸¦æ¥é¢å¤–çš„æ˜¾å­˜å¼€é”€çš„ã€‚ä½†æ˜¯è¿™ç§å¼€é”€åœ¨torch.memory_allocatedä¸­æ˜¯ä¸èƒ½è¢«å¯Ÿè§‰çš„ã€‚  
æ¯”å¦‚åœ¨AdamWåœ¨è¿›è¡ŒæŸä¸€å±‚çš„æ›´æ–°çš„æ—¶å€™ï¼Œä¼šå¸¦æ¥2å€è¯¥å±‚å‚æ•°é‡å¤§å°çš„ä¸´æ—¶é¢å¤–å¼€é”€ã€‚è¿™ä¸ªåœ¨max_memory_allocatedä¸­å¯ä»¥çœ‹åˆ°ã€‚  
  
```python  
print(torch.cuda.memory_summary())  
```  
  
    |===========================================================================|  
    |                  PyTorch CUDA memory summary, device ID 0                 |  
    |---------------------------------------------------------------------------|  
    |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |  
    |===========================================================================|  
    |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |  
    |---------------------------------------------------------------------------|  
    | Allocated memory      |   49152 KB |  106497 KB |  180227 KB |  131074 KB |  
    |       from large pool |   49152 KB |  106496 KB |  180224 KB |  131072 KB |  
    |       from small pool |       0 KB |       2 KB |       3 KB |       2 KB |  
    |---------------------------------------------------------------------------|  
    | Active memory         |   49152 KB |  106497 KB |  180227 KB |  131074 KB |  
    |       from large pool |   49152 KB |  106496 KB |  180224 KB |  131072 KB |  
    |       from small pool |       0 KB |       2 KB |       3 KB |       2 KB |  
    |---------------------------------------------------------------------------|  
    | GPU reserved memory   |  124928 KB |  124928 KB |  124928 KB |       0 B  |  
    |       from large pool |  122880 KB |  122880 KB |  122880 KB |       0 B  |  
    |       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |  
    |---------------------------------------------------------------------------|  
    | Non-releasable memory |   14335 KB |   20479 KB |   28674 KB |   14338 KB |  
    |       from large pool |   12288 KB |   18432 KB |   26624 KB |   14336 KB |  
    |       from small pool |    2047 KB |    2047 KB |    2050 KB |       2 KB |  
    |---------------------------------------------------------------------------|  
    | Allocations           |       7    |       9    |      20    |      13    |  
    |       from large pool |       6    |       8    |      16    |      10    |  
    |       from small pool |       1    |       3    |       4    |       3    |  
    |---------------------------------------------------------------------------|  
    | Active allocs         |       7    |       9    |      20    |      13    |  
    |       from large pool |       6    |       8    |      16    |      10    |  
    |       from small pool |       1    |       3    |       4    |       3    |  
    |---------------------------------------------------------------------------|  
    | GPU reserved segments |       7    |       7    |       7    |       0    |  
    |       from large pool |       6    |       6    |       6    |       0    |  
    |       from small pool |       1    |       1    |       1    |       0    |  
    |---------------------------------------------------------------------------|  
    | Non-releasable allocs |       2    |       3    |       4    |       2    |  
    |       from large pool |       1    |       2    |       3    |       2    |  
    |       from small pool |       1    |       1    |       1    |       0    |  
    |---------------------------------------------------------------------------|  
    | Oversize allocations  |       0    |       0    |       0    |       0    |  
    |---------------------------------------------------------------------------|  
    | Oversize GPU segments |       0    |       0    |       0    |       0    |  
    |===========================================================================|  
  
### æ˜¾å­˜å‰©ä½™é‡è¶³å¤Ÿå´æç¤ºOOM  
  
```shell  
CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 15.78 GiB total capacity; 10.34 GiB already allocated; 435.50 MiB free; 14.21 GiB reserved in total by PyTorch)  
```  
-   **Tried to allocate**ï¼šæŒ‡æœ¬æ¬¡ malloc æ—¶é¢„è®¡åˆ†é…çš„ alloc_sizeï¼›  
-   **total capacity**ï¼šç”± cudaMemGetInfo è¿”å›çš„ device æ˜¾å­˜æ€»é‡ï¼›  
-   **already allocated**ï¼šç”±ç»Ÿè®¡æ•°æ®è®°å½•ï¼Œå½“å‰ä¸ºæ­¢è¯·æ±‚åˆ†é…çš„ size çš„æ€»å’Œï¼›  
-   **free**ï¼šç”± cudaMemGetInfo è¿”å›çš„ device æ˜¾å­˜å‰©ä½™é‡ï¼›  
-   **reserved**ï¼šBlockPool ä¸­æ‰€æœ‰ Block çš„å¤§å°ï¼Œä¸å·²ç»åˆ†é…çš„ Block å¤§å°çš„æ€»å’Œã€‚    
    å³ [reserved] = [already allocated] + [sum size of 2 BlockPools]  
  
å…¶ä¸­ï¼Œpytorch reserved çš„14.21GBä¸­ï¼Œåªæœ‰10.34GBæ˜¯å·²ç»åˆ†é…å‡ºå»çš„ï¼Œå‰©ä¸‹çš„3.9GBæ˜¯ç¢ç‰‡åŒ–çš„æ˜¾å­˜ã€‚  
  
- `export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32`  
> PyTorch è®¤ä¸ºï¼Œä»ç»Ÿè®¡ä¸Šæ¥è¯´å¤§éƒ¨åˆ†å†…å­˜ç”³è¯·éƒ½æ˜¯å°äºæŸä¸ªé˜ˆå€¼çš„ï¼Œè¿™äº›å¤§å°çš„ Block æŒ‰ç…§å¸¸è§„å¤„ç†ï¼Œè¿›è¡Œæ‹†åˆ†ä¸ç¢ç‰‡ç®¡ç†ï¼›ä½†å¯¹å¤§äºé˜ˆå€¼çš„ Block è€Œè¨€ï¼ŒPyTorch è®¤ä¸ºè¿™äº›å¤§çš„ Block ç”³è¯·æ—¶å¼€é”€å¤§ï¼ˆæ—¶é—´ï¼Œå¤±è´¥é£é™©ï¼‰ï¼Œå¯ä»¥ç•™å¾…åˆ†é…ç»™ä¸‹æ¬¡è¾ƒå¤§çš„è¯·æ±‚ï¼Œäºæ˜¯ä¸é€‚åˆæ‹†åˆ†ã€‚é»˜è®¤æƒ…å†µä¸‹é˜ˆå€¼å˜é‡Â `max_split_size_mb`Â ä¸º INT_MAXï¼Œå³å…¨éƒ¨ Block éƒ½å¯ä»¥æ‹†åˆ†ã€‚  
  
![[../assets/img/Pasted image 20230310223749.png|Pasted image 20230310223749.png]]  
> å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå‡è®¾å½“å‰æƒ³åˆ†é… 800MB æ˜¾å­˜ï¼Œè™½ç„¶ç©ºé—²çš„æ€»æ˜¾å­˜æœ‰ 1000MBï¼Œä½†æ˜¯ä¸Šæ–¹å›¾çš„ç©ºé—²æ˜¾å­˜ç”±åœ°å€ä¸è¿ç»­çš„ä¸¤ä¸ª 500MB çš„å—ç»„æˆï¼Œä¸å¤Ÿåˆ†é…è¿™ 800MB æ˜¾å­˜ï¼›è€Œä¸‹æ–¹çš„å›¾ä¸­ï¼Œå¦‚æœä¸¤ä¸ª 500MB çš„ç©ºé—²å—åœ°å€è¿ç»­ï¼Œå°±å¯ä»¥é€šè¿‡æ˜¾å­˜ç¢ç‰‡çš„æ•´ç†ç»„æˆä¸€ä¸ª 1000MB çš„æ•´å—ï¼Œè¶³å¤Ÿåˆ†é… 800MBã€‚ä¸Šæ–¹å›¾çš„è¿™ç§æƒ…å†µå°±è¢«ç§°ä¸º**æ˜¾å­˜ç¢ç‰‡åŒ–**ã€‚  
  
```cardlink  
url: https://zhuanlan.zhihu.com/p/486360176  
title: "ä¸€æ–‡è¯»æ‡‚ PyTorch æ˜¾å­˜ç®¡ç†æœºåˆ¶"  
description: "æœ¬æ–‡é¦–å‘äºçŸ¥ä¹ä¸“æ  è¸¢ç¿»ç‚¼ä¸¹ç‚‰æœ¬æ–‡ä½¿ç”¨çš„ PyTorch æºç ä¸º master åˆ†æ”¯ï¼Œcommit id ä¸º a5b848aec10b15b1f903804308eed4140c5263cbã€‚èƒŒæ™¯ä»‹ç»å‰–æ PyTorch æ˜¾å­˜ç®¡ç†æœºåˆ¶ä¸»è¦æ˜¯ä¸ºäº†å‡å°‘ æ˜¾å­˜ç¢ç‰‡åŒ–å¸¦æ¥çš„å½±å“ã€‚ä¸€ä¸ªâ€¦"  
host: zhuanlan.zhihu.com  
```  
  
## è®¡ç®—æ€§èƒ½ä¼˜åŒ–   
  
> ç›®æ ‡ï¼šæœ€å¤§åŒ–æ•°æ®ååï¼Œæœ€å°åŒ–è®­ç»ƒæ—¶é—´æˆæœ¬ï¼ˆå•ä½æ€§èƒ½æ—¶é—´ï¼‰ï¼›  
  
**`forward`Â vsÂ `backward`Â Execution Speed**  
  
For convolutions and linear layers there are **2x flops** in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and itâ€™s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput).  
  
 > [!TIP]- æ¯”å¦‚x*y=z	  
  
### è¾“å…¥ç«¯   
- æ›´åˆé€‚çš„BatchSize: æ›´å¤§çš„æ‰¹æ¬¡å¤§å°é€šå¸¸ä¼šå¯¼è‡´æ›´å¿«çš„æ¨¡å‹æ”¶æ•›æˆ–æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½;  
- æ•°æ®æµä½¿ç”¨ç”Ÿæˆå™¨ï¼Œæ¯”å¦‚ pytorch çš„ dataloderï¼Œä¹Ÿå¯ä»¥è‡ªå»ºç”Ÿæˆå™¨ï¼›  
  
> -   Choose the batch size and the number of inputs and outputs to be divisible by 4 (TF32) / 8 (FP16) / 16 (INT8) to run efficiently on Tensor Cores. For best efficiency on A100, choose these parameters to be divisible by 32 (TF32) / 64 (FP16) / 128 (INT8) refer toÂ [Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc).  
> - Tensor Cores are most efficient when key parameters of the operation are multiples of 4 if using TF32, 8 if using FP16, or 16 if using INT8 (equivalently, when key dimensions of the operation are aligned to multiples of 16 bytes in memory). For fully-connected layers, the relevant parameters are the batch size and the number of inputs and outputs; for convolutional layers, the number of input and output channels; and for recurrent layers, the minibatch size and hidden sizes.  
   
  
  
### å‰å‘&åå‘è¿‡ç¨‹ï¼ˆä¸­é—´è¿‡ç¨‹ï¼‰  
  
#### æ¢¯åº¦ç´¯è®¡   
- ç´¯è®¡å¤šæ¬¡çš„æ¢¯åº¦å€¼ï¼Œè¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­ï¼ˆæ›´æ–°æ¨¡å‹æƒé‡ï¼‰ï¼Œå¯ä»¥å˜ç›¸å®ç°åœ¨å°çš„æ˜¾å­˜å ç”¨æƒ…å†µä¸‹çš„å¤§çš„Batch Sizeï¼›  
- æ˜¯ä¸€ç§å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹çš„æœ‰æ•ˆæ–¹æ¡ˆï¼›  
- å¦ä¸€ç§è¾¾åˆ°ç±»ä¼¼æ•ˆæœçš„æ‰‹æ®µ [[æ•°æ®å¹¶è¡Œ]]ï¼›  
![[../assets/img/Pasted image 20230311184224.png|Pasted image 20230311184224.png]]  
  
```python  
# æ¢¯åº¦ç´¯åŠ å‚æ•°  
accumulation_steps = 4  
  
  
for i, (images, labels) in enumerate(train_data):  
    # 1. forwared å‰å‘è®¡ç®—  
    outputs = model(imgaes)  
    loss = criterion(outputs, labels)  
  
    # 2.1 loss regularization lossæ­£åˆ™åŒ–  
    loss += loss / accumulation_steps  
  
    # 2.2 backward propagation åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦  
    loss.backward()  
  
    # 3. update parameters of net  
    if ((i+1) % accumulation)==0:  
        # optimizer the net  
        optimizer.step()  
        optimizer.zero_grad() # reset grdient  
```  
å½“ç„¶åœ¨å®é™…å·¥ç¨‹å½“ä¸­ï¼Œå…³äºè°ƒå‚å’Œç®—æ³•ä¸Šæœ‰ä¸¤ç‚¹éœ€è¦æ³¨æ„çš„ï¼š  
  
> **å­¦ä¹ ç‡ learning rate**ï¼šä¸€å®šæ¡ä»¶ä¸‹ï¼ŒBatch sizeè¶Šå¤§è®­ç»ƒæ•ˆæœè¶Šå¥½ï¼Œæ¢¯åº¦ç´¯ç§¯åˆ™æ¨¡æ‹Ÿäº†batch sizeå¢å¤§çš„æ•ˆæœï¼Œå¦‚æœaccumulation stepsä¸º4ï¼Œåˆ™Batch sizeå¢å¤§äº†4å€ï¼Œæ ¹æ®ZOMIçš„ç»éªŒï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯çš„æ—¶å€™éœ€è¦æŠŠå­¦ä¹ ç‡é€‚å½“æ”¾å¤§ã€‚    
> **å½’ä¸€åŒ– Batch Norm**ï¼šaccumulation stepsä¸º4æ—¶è¿›è¡ŒBatch sizeæ¨¡æ‹Ÿæ”¾å¤§æ•ˆæœï¼Œå’ŒçœŸå®Batch sizeç›¸æ¯”ï¼Œæ•°æ®çš„åˆ†å¸ƒå…¶å®å¹¶ä¸å®Œå…¨ç›¸åŒï¼Œ4å€Batch sizeçš„BNè®¡ç®—å‡ºæ¥çš„å‡å€¼å’Œæ–¹å·®ä¸å®é™…æ•°æ®å‡å€¼å’Œæ–¹å·®ä¸å¤ªç›¸åŒï¼Œå› æ­¤æœ‰äº›å®ç°ä¸­ä¼šä½¿ç”¨Group Normæ¥ä»£æ›¿Batch Normã€‚  
  
#### æ¢¯åº¦æ£€æŸ¥ç‚¹(Gradient Checkpoint)  
- ä»¥æ—¶é—´æ¢ç©ºé—´ï¼ŒæŒ‰ç…§ä¸€å®šç­–ç•¥ä¿ç•™ä¸€éƒ¨åˆ†å‰å‘ä¼ æ’­çš„ä¸­é—´å˜é‡ï¼Œå…¶å®ƒå˜é‡åœ¨åå‘ä¼ æ’­æ—¶é€šè¿‡è®¡ç®—è·å¾—ï¼›  
- ä¸€èˆ¬çš„ç»éªŒæ³•åˆ™æ˜¯ï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹ä¼šä½¿è®­ç»ƒé€Ÿåº¦å‡æ…¢çº¦20%ã€‚  
- æŠ€æœ¯æ¥æºï¼š[openai/gradient-checkpointing](https://github.com/openai/gradient-checkpointing)  
![[../assets/img/Pasted image 20230311184931.png|500]]  
  
[Fetching Data#w60h](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)  
  
  
#### æ··åˆç²¾åº¦è®­ç»ƒ  
- [[AMP and tensor core|AMP and tensor core]]  
- æ··åˆç²¾åº¦è®­ç»ƒçš„æƒ³æ³•æ˜¯ï¼Œå¹¶éæ‰€æœ‰å˜é‡éƒ½éœ€è¦ä»¥å®Œæ•´çš„ï¼ˆ32ä½ï¼‰æµ®ç‚¹ç²¾åº¦å­˜å‚¨ã€‚å¦‚æœæˆ‘ä»¬èƒ½é™ä½ç²¾åº¦ï¼Œå˜é‡åŠå…¶è®¡ç®—ä¼šæ›´å¿«ã€‚  
![[../assets/img/Pasted image 20230311221908.png|500]]  
  
####  DeepSpeed ZeRO   
  
#### CPUOffload  
  
  
### æ¨¡å‹ç«¯  
- compileï¼š PyTorch 2.0å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç¼–è¯‘å‡½æ•°ï¼Œæ‚¨å¯ä»¥åœ¨[ä»–ä»¬çš„æ–‡æ¡£ä¸­](https://pytorch.org/get-started/pytorch-2.0/)äº†è§£æ›´å¤šä¿¡æ¯ã€‚å®ƒä½¿ç”¨Pythonçš„å¸§è¯„ä¼°APIä»ç°æœ‰çš„PyTorchç¨‹åºä¸­è‡ªåŠ¨åˆ›å»ºå›¾å½¢ã€‚æ•è·å›¾å½¢åï¼Œå¯ä»¥éƒ¨ç½²ä¸åŒçš„åç«¯ï¼Œå°†å›¾å½¢é™ä½åˆ°ä¼˜åŒ–çš„å¼•æ“ã€‚  
-  `dynamo.optimize("inductor")`Â - Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton kernelsÂ [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)  
-   `dynamo.optimize("nvfuser")`Â - nvFuser with TorchScript.Â [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)  
-   `dynamo.optimize("aot_nvfuser")`Â - nvFuser with AotAutograd.Â [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)  
-   `dynamo.optimize("aot_cudagraphs")`Â - cudagraphs with AotAutograd.Â [Read more](https://github.com/pytorch/torchdynamo/pull/757)  
### ä¼˜åŒ–å™¨   
- ä¼˜åŒ–å™¨é€šè¿‡é‡åŒ–æ¥å‡å°‘å†…å­˜å ç”¨ï¼›  
- A standard AdamW uses 8 bytes for each parameter, here the optimizer will need (`8*3`) 24GB of GPU memory.  
-   Adafactor uses slightly more than 4 bytes, so (`4*3`) 12GB and then some extra.  
-   8bit BNB quantized optimizer will use only (`2*3`) 6GB if all optimizer states are quantized.  
  
  
### é€‰æ‹©æ›´åˆé€‚çš„æ˜¾å¡   
![[../assets/img/Pasted image 20230311181752.png|Pasted image 20230311181752.png]]  
### å…¶å®ƒ   
-   1ã€å¯¹äºæ²¡æœ‰å‚æ•°çš„å±‚å¯ä»¥ä½¿ç”¨ inplace æ“ä½œï¼Œå¦‚åœ¨ pytorch ä¸­ï¼Œrelu æ¿€æ´»å‡½æ•°å°±æœ‰ inplace é€‰é¡¹ã€‚å‡è®¾ x æ˜¯è¾“å…¥ï¼Œy æ˜¯è¾“å‡ºï¼Œè¿™ä¸ªæ“ä½œçš„å«ä¹‰æ˜¯åœ¨è®¡ç®—å‡º y ä¹‹åï¼Œç”¨ y ä»£æ›¿ xã€‚åœ¨è¿›è¡Œæ¢¯åº¦æ›´æ–°çš„æ—¶å€™ï¼Œç”¨ y çš„å€¼è®¡ç®—å‡º x è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚è¿™ä¸ªæ–¹æ³•æ¯”è¾ƒé€‚ç”¨äºç®€å•çš„æ— å‚æ•°å±‚ï¼Œåœ¨è¿‡äºå¤æ‚çš„æƒ…å†µä¸‹ï¼Œç­‰äºç‰ºç‰²æ—¶é—´æˆæœ¬æ¢ç©ºé—´ã€‚  
- 2ã€é€‰æ‹©æ¯”è¾ƒè½»é‡çº§åˆ«çš„ä¼˜åŒ–å™¨ã€‚  
-  3ã€åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œpadding çš„å¤„ç†éå¸¸é‡è¦ã€‚æ— è®ºæ˜¯ CNNï¼ŒRNNï¼Œæˆ‘ä»¬éƒ½åº”å½“ä½¿ç”¨åŠ¨æ€çš„ç»“æ„ï¼ˆåŠ¨æ€rnnï¼Œé¿å…é™æ€ RNNï¼‰æ¥é¿å… padding éƒ¨åˆ†çš„è®¡ç®—ã€‚åœ¨æœ‰äº› nlp ä»»åŠ¡ä¸­ï¼Œä¸æ˜¯æ¯ä¸ªå•è¯çš„è¡¨å¾å±‚éƒ½è¦è¢«ç”¨æ¥ predictï¼Œæˆ‘ä»¬å¯ä»¥åªé€‰æ‹©æœ‰ç”¨çš„è¯æ¥è¿›è¡Œæœ€åé¢„æµ‹å±‚çš„è®¡ç®—ã€‚  
  
### æ€»ç»“   
  
| Method                   | Speed | Memory |  
| ------------------------ | ----- | ------ |  
| Gradient accumulation    | No    | Yes    |  
| Gradient checkpointing   | No    | Yes    |  
| Mixed precision training | Yes   | (No)   |  
| Batch size               | Yes   | Yes    |  
| Optimizer choice         | Yes   | Yes    |  
| DataLoader               | Yes   | No     |  
| DeepSpeed Zero           | No    | Yes    |  
  
  
  
## æ‰©å±•é˜…è¯»ææ–™   
  
```cardlink  
url: https://docs.nvidia.com/deeplearning/performance/index.html  
title: "NVIDIA Deep Learning Performance - NVIDIA Docs"  
description: "GPUs accelerate machine learning operations by performing calculations in parallel. Many operations, especially those representable as matrix multipliers will see good acceleration right out of the box. Even better performance can be achieved by tweaking operation parameters to efficiently use GPU resources. The performance documents present the tips that we think are most widely useful."  
host: docs.nvidia.com  
favicon: /favicon-32x32.png  
```  
  
  
```cardlink  
url: https://huggingface.co/docs/transformers/perf_train_gpu_one  
title: "Efficient Training on a Single GPU"  
description: "Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science."  
host: huggingface.co  
image: https://huggingface.co/front/thumbnails/docs/transformers.png  
```  
### è®¡ç®—   
#### [5. DNN Operation Categories](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#dnn-op-cat)  
  
While modern neural networks are built from a variety of layers, their operations fall into three main categories according to the nature of computation.  
  
##### [5.1.Â Elementwise Operations](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#element-op)  
  
Elementwise operations may be unary or binary operations; the key is that layers in this category perform mathematical operations on each element independently of all other elements in the tensor.  
  
For example, a ReLU layer returnsÂ `max(0,Â _x_)`Â for eachÂ `_x_`Â in the input tensor. Similarly, element-wise addition of two tensors computes each output sum value independently of other sums. Layers in this category include most non-linearities (sigmoid, tanh, etc.), scale, bias, add, and others. These layers tend to be memory-limited, as they perform few operations per byte accessed. Further details on activations, in particular, can be found within theÂ [Activations](https://docs.nvidia.com/deeplearning/performance/dl-performance-memory-bound/index.html#activations)Â section in theÂ _Optimizing Memory-Bound Layers User's Guide_.  
  
##### [5.2.Â Reduction Operations](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#reduction-op)  
  
Reduction operations produce values computed over a range of input tensor values.  
  
For example, pooling layers compute values over some neighborhoods in the input tensor. Batch normalization computes the mean and standard deviation over a tensor before using them in operations for each output element. In addition to pooling and normalization layers, SoftMax also falls into the reduction category. Typical reduction operations have a low arithmetic intensity and thus are memory limited. Further details on pooling layers can be found withinÂ [Pooling](https://docs.nvidia.com/deeplearning/performance/dl-performance-memory-bound/index.html#pooling).  
  
##### [5.3.Â Dot-Product Operations](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#dot-prod-op)  
  
Operations in this category can be expressed as dot-products of elements from two tensors, usually a weight (learned parameter) tensor and an activation tensor.  
  
These include fully-connected layers, occurring on their own and as building blocks of recurrent and attention cells. Fully-connected layers are naturally expressed as matrix-vector and matrix-matrix multiplies. Convolutions can also be expressed as collections of dot-products - one vector is the set of parameters for a given filter, the other is an â€œunrolledâ€ activation region to which that filter is being applied. Since filters are applied in multiple locations, convolutions too can be viewed as matrix-vector or matrix-matrix multiply operations (refer toÂ [Convolution Algorithms](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#conv-algo)).  
  
Operations in the dot-product category can be math-limited if the corresponding matrices are large enough. However, for the smaller sizes, these operations end up being memory-limited. For example, a fully-connected layer applied to a single vector (a tensor for a mini-batch of size 1)) is memory limited. Matrix-matrix multiplication performance is discussed in more detail in theÂ [NVIDIA Matrix Multiplication Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html). Information on modeling a type of layer as a matrix multiplication can be found in the corresponding guides:  
  
-   [NVIDIA Optimizing Linear/Fully-Connected Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html)  
-   [NVIDIA Optimizing Convolutional Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html)  
-   [NVIDIA Optimizing Recurrent Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-recurrent/index.html)