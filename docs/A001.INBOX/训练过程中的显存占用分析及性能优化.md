---  
share: ture  
tags:  
  - notes  
  - 优化  
  - 分析策略  
  - PPT  
links:  
  - - ""  
date: 2023-03-12 13:58  
status: ⭕  
aliases: []  
---  
  
![|banner](../assets/img/23.jpg)  
  
# 训练过程中的显存占用分析及性能优化  
---  
📌 #优化 #分析策略   
 > [!abstract]+   
 > 梳理面相高效训练的显存优化策略。  
  
## 计算性能分析  
![[../assets/img/Pasted image 20230311174306.png|500]]  
- [i] 从整个深度学习的训练过程来看，训练过程中对计算资源的需求多数会导致显存成为最终性能的瓶颈。  
![[../assets/img/Pasted image 20230312140259.png|Pasted image 20230312140259.png]]  
  
 > [!TIP]- 🌰 举个例子  
 > 以VGG16为例：  
 > - 模型的参数量为138M，加载占用显存：138 * 4 / 1024 / 1024 = 528MB   
 > - 如果使用SGD作为优化器，需要额外的参数梯度也为528MB   
 >  - 如果使用Adam，额外的梯度相关数据为 528* 3 MB   
 > - 每层输出所占用的内存和batch size成正比：  
 >  - bs=1,需要58.12MB，而训练过程有forward的password，所以为58.12MB x 2   
 >  - ![](https://upload-images.jianshu.io/upload_images/10780978-c0e762f4c98bcb71.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720)  
  
### 问题分析  
  
![[../assets/img/Diagram 3.svg|500]]  
  
![[../assets/img/gpu_memory.svg|500]]  
  
 > [!FAQ]- 思考  
 > 1）哪个环节显存占用最多？  
 > 2）哪个环节是计算框架/平台可以优化的显存占用？ 哪个环节是用户侧需要主导的？  
 > 3）哪些是训练和推理阶段存在差异的？	  
  
  
### 模型权重  
-   4 bytes * number of parameters for fp32 training  
-   6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)  
  
### 优化器状态   
-   8 bytes * number of parameters for normal AdamW (maintains 2 states)  
-   2 bytes * number of parameters for 8-bit AdamW optimizers like [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)  
-   4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)  
  
### 梯度   
-   4 bytes * number of parameters for either fp32 mixed precision training (gradients are always kept in fp32)  
  
### 前向激活信息(中间结果)  
-   size depends on many factors, the key ones being sequence length, hidden size and batch size.  
- There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation.  
  
### 临时存储   
- Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it’s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.  
  
### 函数有关的内存占用  
Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs.  
  
  
  
 > [!QUOTE]+ 显存消耗数据   
 > 模型训练相关： **对于GPT-2这样1.5B参数的模型，显存消耗至少24GB。**  
 >  中间变量的消耗（激活）：一个具体的例子，模型为1.5B的GPT-2，序列长度为1K，batch size为32，则消耗显存为60GB。  
 >  **临时缓存区(Temporary buffers)**。对于大模型，用于存储中间结果的临时buffer也会消耗大量显存。例如在all-reduce时，需要一个平坦的buffer来融合所有的梯度，从而改善吞吐量。例如，跨设备的all-reduce操作会随着消息的增大而增加。虽然，梯度本文是fp16的张量，但是有些操作中可能需要融合的buffer为fp32。当模型尺寸很大时，临时的buffer也不小。例如，对于1.5B参数的模型，一个fp32的buffer需要6GB的显存。  
 >  **显存碎片**。即使在有足够显存的情况下，也可能会导致Out of Memory，这是由于显存碎片导致的。在进程发出显存请求时，如果没有连续的显存来满足请求，即使总的显存仍然足够，该请求也会失败。当训练非常大的模型时，可以观察到明显的显存碎片。极端情况下，可能会导致30%的显存碎片。  
  
 #PPT   
 http://home.ustc.edu.cn/~fzr/pdf/GPU_memory_analysis.pdf  
 https://www.bilibili.com/read/cv19522369?from=articleDetail  
```cardlink  
url: https://www.bilibili.com/read/cv19522369?from=articleDetail  
title: "不止于ZeRO：BMTrain技术原理浅析"  
description: "与现有的大模型训练使用百余张显卡相比，我们发起的CPM-Live 开源大模型直播训练实现了 8 张 A100 显卡 训练百亿大模型。这优异效果的背后基于的是 大模型高效训练工具 BMTrain 和 模型仓库 ModelCenter。与现有框架相比，BMTrain 能够实现大模型的低资源、高效训练，并且简单易用，便于开发者上手。支撑起 BMTrain 优异性能表现的是其采用的多项分布式训练优化技术，它们共同解决了大模型训练过程中的 显存占用 问题。为了深刻理解这一关键问题，我们不妨分析一下模型训练"  
host: www.bilibili.com  
image: https://i0.hdslb.com/bfs/article/6f3db3c5ccab1ce35f19d8601c6b718ebe37123f.png  
```  
  
  
  
## 基于Pytorch的实验分析  
### 计算工具  
```bash  
pip install torchstat thop  
  
from torchstat import stat   
stat(model, (3, 224, 224))  
```  
  
```python  
from pynvml import *  
  
def print_gpu_utilization():  
    nvmlInit()  
    handle = nvmlDeviceGetHandleByIndex(0)  
    info = nvmlDeviceGetMemoryInfo(handle)  
    print(f"GPU memory occupied: {info.used//1024**2} MB.")  
```  
  
- `torch.cuda.memory_allocated()` 和 `torch.cuda.max_memory_allocated()`  
- 前者可以精准地反馈当前进程中Torch.Tensor所占用的GPU显存，后者则可以告诉我们到调用函数为止所达到的最大的显存占用字节数。  
 -   使用torch.cuda.memory_allocated() 可以看到当前Tensor占用的显存  
 -   使用 torch.cuda.memory_reserved() 可以看到总共占用的显存  
 -   使用 torch.cuda.empty_cache() 清空未使用的缓存，但是已经使用的是不能释放  
- torchsummary 和 torchinfo都可以展现模型的大小信息  
### Pytorch中显存占用分析  
  
- [?] nvidia-smi中看到的占用 = `CUDA Context + pytorch的缓存区` = `CUDA Context + 未使用的缓存 + 已使用的缓存`   
  
在PyTorch中，显存是按页为单位进行分配的，这可能是CUDA设备的限制。就算我们只想申请4字节的显存，pytorch也会先向CUDA设备申请2MB的显存到自己的cache区中，然后pytorch再为我们分配512字节或者1024字节的空间。这个在使用`torch.cuda.memory_allocated()`的时候可以看出来512字节；用`torch.cuda.memory_cached()`可以看出向CUDA申请的2MB。直观点来说，看图吧，PyTorch的显存管理是一个层级结构。  
在PyTorch中，只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存。  
![[../assets/img/Pasted image 20230108101533.png|450]]  
  
> Pytorch的机制是使用缓存分配器来管理缓存分配的(因为这样速度快), 但是在缓存分配器的机制下, **一个Tensor就算被释放了，进程也不会把空闲出来的显存还给GPU，而是等待下一个Tensor来填入这一片被释放的空间(即只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存，所以在nvidia-smi/gpustat中看到的显存并没有减少**  
  
  
 > [!FAQ]+ 混合精度计算中Zero论文中的计算公式Adam的优化器状态为12B  
>  因为使用了混合精度，包括模型参数（fp16）、模型梯度（fp16）和Adam状态（fp32的模型参数备份，fp32的momentum和fp32的variance）；  
  >  ![[../assets/img/Pasted image 20230108115706.png|Pasted image 20230108115706.png]]  
  
 > [!TIP]- CUDA Context  
 > 就是在第一次执行CUDA操作，也就是使用GPU的时候所需要创建的维护设备间工作的一些相关信息。  
 > 这个值跟CUDA的版本，pytorch的版本以及所使用的设备都是有关系的。比如3090用的CUDA 11.4，开销为1639MB；V100用的CUDA 10.2，开销为1351MB。	  
  
### 实验脚本  
  
1. 模型定义 --> 2. 前向传播 --> 3.反向传播 -->  4.参数更新  
  
  
```python  
import torch  
```  
  
#### 分配一个简单的Tensor的情况  
将显存从nvidia-smi中释放：  
```python  
torch.cuda.empty_cache()  
```  
  
```python  
print(torch.cuda.memory_allocated())  
```  
    0  
  
```python  
temp = torch.tensor([1.0]).cuda()  
```  
页分配: Pytorch有自己的显存管理系统，通过预留和页分配来管理内存  
  
```python  
print(torch.cuda.memory_allocated())  
```  
  
    512  
  
CUDA Context： 和不同的显卡、计算框架的版本有关(执行一个简单指令之后，查看nvidia-smi看实际分配值，减去`torch.cuda.memory_reserved() `的值)， 可能是个动态的值大概600多MB-1GB  
  
不同的pytorch函数，显卡型号，驱动，操作系统，cuda版本都是会影响context_memory大小的。  
  
```python  
torch.cuda.memory_reserved()  
```  
    2097152  
```python  
torch.cuda.memory_reserved() / 1024 / 1024  
```  
    2.0  
  
```python  
del temp  
```  
  
```python  
torch.cuda.empty_cache()  
```  
```python  
print(torch.cuda.memory_allocated())  
```  
  
    512  
  
  
#### 一个模型的训练过程中的显存数据  
  
  
```python  
import torch  
```  
  
```python  
current_mem = 0  
```  
  
```python  
def get_mem_total():  
    global current_mem  
    current_mem = torch.cuda.memory_allocated() / 1024 / 1024  
    return current_mem  
```  
  
  
```python  
get_mem_total()  
```  
  
  
    0.0  
  
  
```python  
def get_mem_diff():  
    old = current_mem  
    total = get_mem_total()  
    return (total - old)  
```  
  
  
```python  
class Model(torch.nn.Module):  
    def __init__(self):  
        super().__init__()  
        self.model = torch.nn.Linear(1024, 1024, bias=False, dtype=torch.float16)  
      
    def forward(self, x):  
        for i in range(10):  
            x = self.model(x)  
        return x  
```  
  
  
```python  
model = Model().cuda()  
```  
  
  
```python  
print(f"Model parameters: {get_mem_diff()} MB" )  
```  
  
    Model parameters: 2.0 MB  
  
  
  
```python  
inputs = torch.zeros((10240, 1024),  dtype=torch.float16).cuda()  
```  
  
  
```python  
print(f"Inputs: {get_mem_diff()} MB" )  
```  
  
    Inputs: 20.0 MB  
  
  
  
```python  
outputs = model(inputs)   
```  
  
  
```python  
print(f"Outpus and buffers: {get_mem_diff()} MB" )  
```  
  
    Outpus and buffers: 40.0 MB  
  
  
  
```python  
loss = torch.sum(outputs) # + 512(最小分配单位)  
```  
  
  
```python  
get_mem_total()  
```  
  
  
  
  
    62.00048828125  
  
  
  
  
```python  
loss.backward()  
```  
  
  
```python  
print(f"After backward: {get_mem_total()} MB")  
```  
  
    After backward: 44.00048828125 MB  
  
  
包括：20MB的输入信息，20MB的输出，以及2MB的模型参数，2MB的模型参数权重，还有一些其它临时变量  
  
  
```python  
with torch.no_grad():  
    outputs = model(inputs)   
```  
  
  
```python  
print(f"Use a eval mode for predict: {get_mem_diff()} MB")  
```  
  
    Use a eval mode for predict: 0.0 MB  
  
  
  
```python  
!pip install torchinfo  
```  
  
  
```python  
from torchinfo import summary  
```  
  
```python  
summary(model)  
```  
  
    =================================================================  
    Layer (type:depth-idx)                   Param #  
    =================================================================  
    Model                                    --  
    ├─Linear: 1-1                            1,048,576  
    =================================================================  
    Total params: 1,048,576  
    Trainable params: 1,048,576  
    Non-trainable params: 0  
    =================================================================  
  
  
```python  
summary(model, (10240, 1024), dtypes=[torch.float16])  
```  
  
    /home/admin/.local/lib/python3.8/site-packages/torchinfo/torchinfo.py:395: UserWarning: Half precision is not supported with input_size parameter, and may output incorrect results. Try passing input_data directly.  
      warnings.warn(  
      
    ==========================================================================================  
    Layer (type:depth-idx)                   Output Shape              Param #  
    ==========================================================================================  
    Model                                    [10240, 1024]             --  
    ├─Linear: 1-1                            [10240, 1024]             1,048,576  
    ├─Linear: 1-2                            [10240, 1024]             (recursive)  
    ├─Linear: 1-3                            [10240, 1024]             (recursive)  
    ==========================================================================================  
    Total params: 1,048,576  
    Trainable params: 1,048,576  
    Non-trainable params: 0  
    Total mult-adds (T): 1.07  
    ==========================================================================================  
    Input size (MB): 20.97  
    Forward/backward pass size (MB): 41.94  
    Params size (MB): 2.10  
    Estimated Total Size (MB): 65.01  
    ==========================================================================================  
  
#### 引入优化器  
  
如下Adam引入了两个新的状态参数：动量和均值  
  
SGD基础上，为每个参数梯度增加了一阶动量（momentum）和二阶动量（variance）  
  
  
```python  
optimizer = torch.optim.Adam(model.parameters())  
```  
  
  
```python  
optimizer.step()  
```  
  
  
```python  
print(f"Optimizer: {get_mem_diff()} MB" )  
```  
  
    Optimizer: 4.0 MB  
  
```python  
torch.cuda.max_memory_allocated()  
```  
  
由于计算机计算的特性，有一些计算操作在计算过程中是会带来额外的显存开销的。但是这种开销在torch.memory_allocated中是不能被察觉的。  
比如在AdamW在进行某一层的更新的时候，会带来2倍该层参数量大小的临时额外开销。这个在max_memory_allocated中可以看到。  
  
```python  
print(torch.cuda.memory_summary())  
```  
  
    |===========================================================================|  
    |                  PyTorch CUDA memory summary, device ID 0                 |  
    |---------------------------------------------------------------------------|  
    |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |  
    |===========================================================================|  
    |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |  
    |---------------------------------------------------------------------------|  
    | Allocated memory      |   49152 KB |  106497 KB |  180227 KB |  131074 KB |  
    |       from large pool |   49152 KB |  106496 KB |  180224 KB |  131072 KB |  
    |       from small pool |       0 KB |       2 KB |       3 KB |       2 KB |  
    |---------------------------------------------------------------------------|  
    | Active memory         |   49152 KB |  106497 KB |  180227 KB |  131074 KB |  
    |       from large pool |   49152 KB |  106496 KB |  180224 KB |  131072 KB |  
    |       from small pool |       0 KB |       2 KB |       3 KB |       2 KB |  
    |---------------------------------------------------------------------------|  
    | GPU reserved memory   |  124928 KB |  124928 KB |  124928 KB |       0 B  |  
    |       from large pool |  122880 KB |  122880 KB |  122880 KB |       0 B  |  
    |       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |  
    |---------------------------------------------------------------------------|  
    | Non-releasable memory |   14335 KB |   20479 KB |   28674 KB |   14338 KB |  
    |       from large pool |   12288 KB |   18432 KB |   26624 KB |   14336 KB |  
    |       from small pool |    2047 KB |    2047 KB |    2050 KB |       2 KB |  
    |---------------------------------------------------------------------------|  
    | Allocations           |       7    |       9    |      20    |      13    |  
    |       from large pool |       6    |       8    |      16    |      10    |  
    |       from small pool |       1    |       3    |       4    |       3    |  
    |---------------------------------------------------------------------------|  
    | Active allocs         |       7    |       9    |      20    |      13    |  
    |       from large pool |       6    |       8    |      16    |      10    |  
    |       from small pool |       1    |       3    |       4    |       3    |  
    |---------------------------------------------------------------------------|  
    | GPU reserved segments |       7    |       7    |       7    |       0    |  
    |       from large pool |       6    |       6    |       6    |       0    |  
    |       from small pool |       1    |       1    |       1    |       0    |  
    |---------------------------------------------------------------------------|  
    | Non-releasable allocs |       2    |       3    |       4    |       2    |  
    |       from large pool |       1    |       2    |       3    |       2    |  
    |       from small pool |       1    |       1    |       1    |       0    |  
    |---------------------------------------------------------------------------|  
    | Oversize allocations  |       0    |       0    |       0    |       0    |  
    |---------------------------------------------------------------------------|  
    | Oversize GPU segments |       0    |       0    |       0    |       0    |  
    |===========================================================================|  
  
### 显存剩余量足够却提示OOM  
  
```shell  
CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 15.78 GiB total capacity; 10.34 GiB already allocated; 435.50 MiB free; 14.21 GiB reserved in total by PyTorch)  
```  
-   **Tried to allocate**：指本次 malloc 时预计分配的 alloc_size；  
-   **total capacity**：由 cudaMemGetInfo 返回的 device 显存总量；  
-   **already allocated**：由统计数据记录，当前为止请求分配的 size 的总和；  
-   **free**：由 cudaMemGetInfo 返回的 device 显存剩余量；  
-   **reserved**：BlockPool 中所有 Block 的大小，与已经分配的 Block 大小的总和。    
    即 [reserved] = [already allocated] + [sum size of 2 BlockPools]  
  
其中，pytorch reserved 的14.21GB中，只有10.34GB是已经分配出去的，剩下的3.9GB是碎片化的显存。  
  
- `export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32`  
> PyTorch 认为，从统计上来说大部分内存申请都是小于某个阈值的，这些大小的 Block 按照常规处理，进行拆分与碎片管理；但对大于阈值的 Block 而言，PyTorch 认为这些大的 Block 申请时开销大（时间，失败风险），可以留待分配给下次较大的请求，于是不适合拆分。默认情况下阈值变量 `max_split_size_mb` 为 INT_MAX，即全部 Block 都可以拆分。  
  
![[../assets/img/Pasted image 20230310223749.png|Pasted image 20230310223749.png]]  
> 如上图所示，假设当前想分配 800MB 显存，虽然空闲的总显存有 1000MB，但是上方图的空闲显存由地址不连续的两个 500MB 的块组成，不够分配这 800MB 显存；而下方的图中，如果两个 500MB 的空闲块地址连续，就可以通过显存碎片的整理组成一个 1000MB 的整块，足够分配 800MB。上方图的这种情况就被称为**显存碎片化**。  
  
```cardlink  
url: https://zhuanlan.zhihu.com/p/486360176  
title: "一文读懂 PyTorch 显存管理机制"  
description: "本文首发于知乎专栏 踢翻炼丹炉本文使用的 PyTorch 源码为 master 分支，commit id 为 a5b848aec10b15b1f903804308eed4140c5263cb。背景介绍剖析 PyTorch 显存管理机制主要是为了减少 显存碎片化带来的影响。一个…"  
host: zhuanlan.zhihu.com  
```  
  
## 计算性能优化   
  
> 目标：最大化数据吞吐，最小化训练时间成本（单位性能时间）；  
  
**`forward` vs `backward` Execution Speed**  
  
For convolutions and linear layers there are **2x flops** in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput).  
  
 > [!TIP]- 比如x*y=z	  
  
### 输入端   
- 更合适的BatchSize: 更大的批次大小通常会导致更快的模型收敛或更好的最终性能;  
- 数据流使用生成器，比如 pytorch 的 dataloder，也可以自建生成器；  
  
> -   Choose the batch size and the number of inputs and outputs to be divisible by 4 (TF32) / 8 (FP16) / 16 (INT8) to run efficiently on Tensor Cores. For best efficiency on A100, choose these parameters to be divisible by 32 (TF32) / 64 (FP16) / 128 (INT8) refer to [Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc).  
> - Tensor Cores are most efficient when key parameters of the operation are multiples of 4 if using TF32, 8 if using FP16, or 16 if using INT8 (equivalently, when key dimensions of the operation are aligned to multiples of 16 bytes in memory). For fully-connected layers, the relevant parameters are the batch size and the number of inputs and outputs; for convolutional layers, the number of input and output channels; and for recurrent layers, the minibatch size and hidden sizes.  
   
  
  
### 前向&后向过程（中间过程）  
  
#### 梯度累计   
- 累计多次的梯度值，进行一次后向传播（更新模型权重），可以变相实现在小的显存占用情况下的大的Batch Size；  
- 是一种内存不足的情况下的有效方案；  
- 另一种达到类似效果的手段 [[数据并行]]；  
![[../assets/img/Pasted image 20230311184224.png|Pasted image 20230311184224.png]]  
  
```python  
# 梯度累加参数  
accumulation_steps = 4  
  
  
for i, (images, labels) in enumerate(train_data):  
    # 1. forwared 前向计算  
    outputs = model(imgaes)  
    loss = criterion(outputs, labels)  
  
    # 2.1 loss regularization loss正则化  
    loss += loss / accumulation_steps  
  
    # 2.2 backward propagation 反向传播计算梯度  
    loss.backward()  
  
    # 3. update parameters of net  
    if ((i+1) % accumulation)==0:  
        # optimizer the net  
        optimizer.step()  
        optimizer.zero_grad() # reset grdient  
```  
当然在实际工程当中，关于调参和算法上有两点需要注意的：  
  
> **学习率 learning rate**：一定条件下，Batch size越大训练效果越好，梯度累积则模拟了batch size增大的效果，如果accumulation steps为4，则Batch size增大了4倍，根据ZOMI的经验，使用梯度累积的时候需要把学习率适当放大。    
> **归一化 Batch Norm**：accumulation steps为4时进行Batch size模拟放大效果，和真实Batch size相比，数据的分布其实并不完全相同，4倍Batch size的BN计算出来的均值和方差与实际数据均值和方差不太相同，因此有些实现中会使用Group Norm来代替Batch Norm。  
  
#### 梯度检查点(Gradient Checkpoint)  
- 以时间换空间，按照一定策略保留一部分前向传播的中间变量，其它变量在反向传播时通过计算获得；  
- 一般的经验法则是，梯度检查点会使训练速度减慢约20%。  
- 技术来源：[openai/gradient-checkpointing](https://github.com/openai/gradient-checkpointing)  
![[../assets/img/Pasted image 20230311184931.png|500]]  
  
[Fetching Data#w60h](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)  
  
  
#### 混合精度训练  
- [[AMP and tensor core|AMP and tensor core]]  
- 混合精度训练的想法是，并非所有变量都需要以完整的（32位）浮点精度存储。如果我们能降低精度，变量及其计算会更快。  
![[../assets/img/Pasted image 20230311221908.png|500]]  
  
####  DeepSpeed ZeRO   
  
#### CPUOffload  
  
  
### 模型端  
- compile： PyTorch 2.0引入了一个新的编译函数，您可以在[他们的文档中](https://pytorch.org/get-started/pytorch-2.0/)了解更多信息。它使用Python的帧评估API从现有的PyTorch程序中自动创建图形。捕获图形后，可以部署不同的后端，将图形降低到优化的引擎。  
-  `dynamo.optimize("inductor")` - Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton kernels [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)  
-   `dynamo.optimize("nvfuser")` - nvFuser with TorchScript. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)  
-   `dynamo.optimize("aot_nvfuser")` - nvFuser with AotAutograd. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)  
-   `dynamo.optimize("aot_cudagraphs")` - cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)  
### 优化器   
- 优化器通过量化来减少内存占用；  
- A standard AdamW uses 8 bytes for each parameter, here the optimizer will need (`8*3`) 24GB of GPU memory.  
-   Adafactor uses slightly more than 4 bytes, so (`4*3`) 12GB and then some extra.  
-   8bit BNB quantized optimizer will use only (`2*3`) 6GB if all optimizer states are quantized.  
  
  
### 选择更合适的显卡   
![[../assets/img/Pasted image 20230311181752.png|Pasted image 20230311181752.png]]  
### 其它   
-   1、对于没有参数的层可以使用 inplace 操作，如在 pytorch 中，relu 激活函数就有 inplace 选项。假设 x 是输入，y 是输出，这个操作的含义是在计算出 y 之后，用 y 代替 x。在进行梯度更新的时候，用 y 的值计算出 x 进行梯度更新。这个方法比较适用于简单的无参数层，在过于复杂的情况下，等于牺牲时间成本换空间。  
- 2、选择比较轻量级别的优化器。  
-  3、在自然语言处理任务中，padding 的处理非常重要。无论是 CNN，RNN，我们都应当使用动态的结构（动态rnn，避免静态 RNN）来避免 padding 部分的计算。在有些 nlp 任务中，不是每个单词的表征层都要被用来 predict，我们可以只选择有用的词来进行最后预测层的计算。  
  
### 总结   
  
| Method                   | Speed | Memory |  
| ------------------------ | ----- | ------ |  
| Gradient accumulation    | No    | Yes    |  
| Gradient checkpointing   | No    | Yes    |  
| Mixed precision training | Yes   | (No)   |  
| Batch size               | Yes   | Yes    |  
| Optimizer choice         | Yes   | Yes    |  
| DataLoader               | Yes   | No     |  
| DeepSpeed Zero           | No    | Yes    |  
  
  
  
## 扩展阅读材料   
  
```cardlink  
url: https://docs.nvidia.com/deeplearning/performance/index.html  
title: "NVIDIA Deep Learning Performance - NVIDIA Docs"  
description: "GPUs accelerate machine learning operations by performing calculations in parallel. Many operations, especially those representable as matrix multipliers will see good acceleration right out of the box. Even better performance can be achieved by tweaking operation parameters to efficiently use GPU resources. The performance documents present the tips that we think are most widely useful."  
host: docs.nvidia.com  
favicon: /favicon-32x32.png  
```  
  
  
```cardlink  
url: https://huggingface.co/docs/transformers/perf_train_gpu_one  
title: "Efficient Training on a Single GPU"  
description: "We’re on a journey to advance and democratize artificial intelligence through open source and open science."  
host: huggingface.co  
image: https://huggingface.co/front/thumbnails/docs/transformers.png  
```  
### 计算   
#### [5. DNN Operation Categories](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#dnn-op-cat)  
  
While modern neural networks are built from a variety of layers, their operations fall into three main categories according to the nature of computation.  
  
##### [5.1. Elementwise Operations](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#element-op)  
  
Elementwise operations may be unary or binary operations; the key is that layers in this category perform mathematical operations on each element independently of all other elements in the tensor.  
  
For example, a ReLU layer returns `max(0, _x_)` for each `_x_` in the input tensor. Similarly, element-wise addition of two tensors computes each output sum value independently of other sums. Layers in this category include most non-linearities (sigmoid, tanh, etc.), scale, bias, add, and others. These layers tend to be memory-limited, as they perform few operations per byte accessed. Further details on activations, in particular, can be found within the [Activations](https://docs.nvidia.com/deeplearning/performance/dl-performance-memory-bound/index.html#activations) section in the _Optimizing Memory-Bound Layers User's Guide_.  
  
##### [5.2. Reduction Operations](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#reduction-op)  
  
Reduction operations produce values computed over a range of input tensor values.  
  
For example, pooling layers compute values over some neighborhoods in the input tensor. Batch normalization computes the mean and standard deviation over a tensor before using them in operations for each output element. In addition to pooling and normalization layers, SoftMax also falls into the reduction category. Typical reduction operations have a low arithmetic intensity and thus are memory limited. Further details on pooling layers can be found within [Pooling](https://docs.nvidia.com/deeplearning/performance/dl-performance-memory-bound/index.html#pooling).  
  
##### [5.3. Dot-Product Operations](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#dot-prod-op)  
  
Operations in this category can be expressed as dot-products of elements from two tensors, usually a weight (learned parameter) tensor and an activation tensor.  
  
These include fully-connected layers, occurring on their own and as building blocks of recurrent and attention cells. Fully-connected layers are naturally expressed as matrix-vector and matrix-matrix multiplies. Convolutions can also be expressed as collections of dot-products - one vector is the set of parameters for a given filter, the other is an “unrolled” activation region to which that filter is being applied. Since filters are applied in multiple locations, convolutions too can be viewed as matrix-vector or matrix-matrix multiply operations (refer to [Convolution Algorithms](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#conv-algo)).  
  
Operations in the dot-product category can be math-limited if the corresponding matrices are large enough. However, for the smaller sizes, these operations end up being memory-limited. For example, a fully-connected layer applied to a single vector (a tensor for a mini-batch of size 1)) is memory limited. Matrix-matrix multiplication performance is discussed in more detail in the [NVIDIA Matrix Multiplication Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html). Information on modeling a type of layer as a matrix multiplication can be found in the corresponding guides:  
  
-   [NVIDIA Optimizing Linear/Fully-Connected Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html)  
-   [NVIDIA Optimizing Convolutional Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html)  
-   [NVIDIA Optimizing Recurrent Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-recurrent/index.html)